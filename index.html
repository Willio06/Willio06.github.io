<!DOCTYPE html>
<html>
<head>
  <title>Tuur Willio</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    @font-face { font-family: GROUPE; src: url('fonts/groupemedium-8mxgn.otf'); } 
    @font-face { font-family: Gorgone; src: url('fonts/Gorgonedemo-YzaOL.otf'); } 
    @font-face { font-family: RobotoThin; src: url('fonts/Roboto-Thin.ttf'); }
    @font-face { font-family: Roboto; src: url('fonts/Roboto-Regular.ttf'); }
    body {
    background-image: url('img/website_back.png');
    background-size: cover;
    background-position: center;
    background-repeat: no-repeat;
}

    #grad1 {
      background-image: linear-gradient(to top, rgba(255,0,0,0), rgba(255,255,255,1));
    }
    #grad2 {
      background-image: radial-gradient(farthest-corner at 50% 45%, rgba(255,255,255,1)20%, rgba(255,0,0,0)90%)
    }
    h1{
      font-family: GROUPE,sans-serif;
      font-size: 75px;
      text-shadow:  4px 2px 2px rgba(70, 70, 70, 0.5);
    }
    @media (max-width: 768px) {
      h1{
        font-size: 50px;
        text-shadow:  1px 1px 2px rgba(70, 70, 70, 0.5);
    }
    div{
      font-size: 10px;
    }
    }
    div{
      font-family: RobotoThin;
      font-size: 20px;
    }
    table {
  width: 100%;
  table-layout: fixed;
}
#headgap {
  height: 175px; /* Initially matches the header height */
  transition: height 0.3s ease; /* Smooth transition */
}
/* Styles for the header sides */
.header_side {
  width: 30%;
  text-align: center;
}

/* Middle column for the title and navigation */
.header_middle {
  max-width: fit-content;
  text-align: center;
  display: inline-block;
}
    .sticky-header{
        position: fixed;
        flex-direction: column; /* Stack title and navigation vertically */
        top: 0;
        left: 0;
        width: 100%;
        color: rgb(69, 67, 67);
        align-items: center;
        text-align: center;
        margin-top: 0px;
        padding: 0px;
        z-index: 1000; /* Keeps the header on top of other content */
    }

    .sticky-header nav {
        flex-grow: 1;
        
    }
    .sticky-header h1{
          margin-bottom: 5px; /* Reduce the space below the title */
      }

    .nav-buttons {
          margin-top: 0; /* Remove any default top margin from the nav */
          padding-top: 0; 
          list-style: none; /* Remove default list styles */
          margin: 0;
          padding: 0;
          display: flex;
          justify-content: center; /* Horizontally center the buttons */
          align-items: center; /* Vertically center the buttons */
          gap: 25px; /* Add spacing between buttons */
          height: 100%; /* Ensure it fills the header's height */
      }

      .nav-buttons li {
          display: flex; /* Ensure items align within their flex parent */
          justify-content: center;
          align-items: center;
      }

    .nav-buttons button {
        font-family: GROUPE, sans-serif;
        font-size: 16px;
        background-color: transparent; /* Transparent background to match design */
        color: rgb(69, 67, 67); /* Button text color */
        border: none; /* Remove default button border */
        cursor: pointer; /* Change cursor to pointer */
        transition: color 0.3s ease; /* Smooth hover transition */
        padding: 5px 10px; /* Add some padding for better click area */
        text-shadow:  1px 1px 2px rgba(96, 72, 72, 0.5);

    }

    .nav-buttons button:hover {
        color: #45405f; /* Change color on hover */
    }

    @keyframes fade-out {
        from {
          opacity: 100%;
        }
        to {
          opacity: 0%;
        }
      }

      @keyframes fade-in {
        from {
          opacity: 0%;
        }
        to {
          opacity: 100%;
        }
      }
    /*---------------------FOOTER-----------------------------------------------------------*/
    .footer-container a svg {
        width: 24px;
        height: 24px;
    }
    .footer-container a:hover svg {
        fill: #45405f;
    }

    .footer-container {
        display: flex;
        flex-direction: column;
        align-items: center;
        padding: 20px 16px;
        background: linear-gradient(145deg, rgb(202,202,202,0.8), #a4b7c2); /* Subtle gradient for a 3D look */
        box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.25); /* Soft shadow around the footer */
        border-radius: 12px; /* Rounded corners for a smoother look */
        position: relative; /* Ensures it looks like an overlay */
        margin: 20px 16px;
      }

      .footer-container a {
        color: #f5f6f7;
        text-decoration: none;
        font-family: GROUPE;
        font-weight: bold;
        text-align: center;
        text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.5); /* Slight shadow for text depth */
      }

      .footer-container ul {
        display: flex;
        gap: 24px;
        list-style-type: none;
        padding: 0;
        justify-content: center;
        margin: 10px 0;
      }

      .footer-container ul li {
        transition: transform 0.2s, box-shadow 0.2s; /* Add hover effects for interactivity */
      }

      .footer-container ul li svg path:hover {
        transform: translateY(-2px); /* Slight lift on hover */
        box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.3); /* Enhance shadow on hover */
      }

      .footer-container p {
        margin: 0;
        color: white;
        text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.5); /* Add shadow for depth */
      }


    /*-----------------------------------------------------------------------------------------------------*/
    </style> 
</head>
<body>
<header class="sticky-header">
  <table style="width:100%" id="grad1">  <tr>
    <td class="header_side"></td>
    <td id="grad2">  <h1><span>Tuur Willio</span></h1>
      <nav class="nav-bar" >
        <ul class="nav-buttons">
          <li><button onclick="navigateTo('frontpage')">About</button></li>
          <li><button onclick="navigateTo('frontpage')">Education</button></li>
          <li><button onclick="navigateTo('page2')">Experience</button></li>
          <li><button onclick="navigateTo('contact')">Skills</button></li>
        </ul>
      </nav></td>
    <td class="header_side"></td>
  </tr></table>


</header>
<div id="headgap"></div>
<div id="frontpage" >
  In literatuur wordt er naar dit theorema verwezen als het feit dat verzameling van functies
  beschreven door neurale netwerken dicht is in de verzameling van continue functies op een
  compacte set. De implicatie die het geeft, is wat er zo belangrijk is voor neurale netwerken:
  alle continue functies kunnen willekeurig dicht benaderd worden door een neuraal netwerk
  op een compacte verzameling.
  3.5 Universal Approximatie Stelling 14
  3.5.1 Sigmoidale functies
  In deze sectie laten we hier een visueel bewijs van zien op basis van [?]. Deze was wel met
  de beperking van sigmoidale functies waarvoor geldt:
  lim
  x→∞ f (x) = 1 lim
  x→−∞ f (x) = 0 (8)
  Als men spreekt over hyperparameters binnen de context van machine learning, bedoelt men
de parameters die gekozen worden voordat het leerproces begonnen is.Dit zijn dan parame-
ters zoals: de topologie van het netwerk, de learning rate, de cost functie, de activatiefuncties
of zelfs convergentie methoden (variaties op gradi¨ent descent). Deze hebben dan ook enkel
invloed op het leerproces, en geen invloed meer op het netwerk zelf. De juiste parameters
kiezen is essentieel om niet alleen het leerproces te versnellen, maar ook de kwaliteit van
het netwerk te verhogen. Het manueel tunen van deze parameters kan nogal wat tijd in
beslag nemen en zal vaak niet het beste of zelfs een goed resultaat opleveren. Daarom zal
in dit hoofdstuk gekeken worden, om met behulp van verscheidene methodes dit proces te
automatiseren en optimaliseren.
Formeel gezien, beschouw Θ de hyperparameterruimte. Dit is het cartesisch product van
de ruimtes waar elke hyperparameter zich in bevindt. Nu zoekt men een θ ∈ Θ zodat
θ = argmin f(ˆθ)
ˆθ∈Θ
. De functie f is hier de loss functie van het neuraal netwerk dat een opti-
male set hyperparameters nodig heeft.
Hier onder zullen verschillende methodes besproken worden, die gebruikt worden om een
maxima te vinden binnen deze hyperparameterruimte.
4.2 Grid search
Grid search is een methode waarbij men itereert over de verschillende hyperparametercom-
binaties binnen (een deel van) de parameterruimte. Vervolgens kijkt men welke hyperpa-
rametercombinatie de laagste cost heeft. Dit algoritme is rekenkundig zeer intensief, maar
het proces is parallelliseerbaar. Het algoritme is,zoals random search, niet erg effici¨ent.
4.3 Random Search
Random search is een alternatieve methode voor hyperparameteroptimalisatie. In tegen-
stelling tot grid search, waarbij alle mogelijke combinaties van hyperparameters worden
uitgeprobeerd, kiest random search willekeurig verschillende combinaties om te evalueren.
Wederom is deze methode enorm parallelliseerbaar aangezien geen enkele uitvoering van
random search afhangt van de volgende of vorige.
4.4 Bayesiaanse Optimalisatie 34
4.4 Bayesiaanse Optimalisatie
Bayesiaanse optimalisatie is een methode om de extreme waarden te vinden van een moeilijk
te evalueren ’black box’ functie f (x), waar traditionele optimalisatiemethodes niet werken.
Het vergt geen kennis van de afgeleide van de hypothetische functie. Er zijn verschillende
varianten op deze methode. Wetende dat de functie moeilijk te evalueren is, werkt het
algoritme op zo weinig mogelijk ge¨evalueerde punten om de extreme waarde op een comp
  Deze eigenschap is zeer belangrijk omdat men hiermee de functie kan vormen tot een trap
  functie (zie figuur 4), die nodig zal zijn in het bewijs. We zullen later aantonen dat dit idee
  uitgebreid kan worden tot functies verschillend van de sigmoidale functies.
  Het idee achter het bewijs is om voor elke functie een benaderende trapfunctie te construeren,
  net zoals de Riemann sommen een integraal benaderen.
  Met de transformatie uit figuur 4 hebben we al een trap kunnen vormen. Om een tweede
  trap te vormen moeten we eerst een afstand cre¨eren. Als referentie punt pakken we, voor
  de sigmoid functie σ(x) = 1
  1+e−x , even het punt waar σ(ax) = 1/2. Dit komt neer op het
  punt waar geldt e−ax = 1 en dus x = 0. Voor σ(ax + b) = 1/2 krijgen we uiteindelijk
  x = − b
  a . We kunnen a en b dus zo veranderen, zodat we een ruimte van grootte b
  a cre¨eren
  tussen functies σ(ax) en σ(ax + b). Of algemener; een afstand van |b1−b0|
  |a| voor σ(ax + b0)
  en σ(ax + b1). Merk op dat dit een lineaire transformatie is binnen de sigmoid functie, net
  zoals de vector berekeningen voor een neuraal netwerk binnen de activatiefunctie. En dus
  de functies s0,1(x) = σ(ax + b0,1) kunnen we beschouwen als de twee eerste hidden nodes, na
  input, met dezelfde weight a en verschillende biases b0, b1. En deze a speelt enkel een rol in
  het meer hoekig maken van de functies.
  3.5 Universal Approximatie Stelling 15
  Figuur 4: De sigmoid functie met verschillende input
  Men kan dan s0 en s1 vermenigvuldigen met een gewenste traphoogte (dit kan door sigmoidale
  activatiefunctie, ze hebben immers hoogte gaande naar 1), w0 en w1 respectievelijk. Optelling
  met elkaar levert dan weer een trapfunctie met een extra trap. Dit weer gestuurd door de
  sigmoid functie als activatie functie, zou de output zijn van een neuraal netwerk met een
  hidden layer die twee nodes (s0 en s1) bevat, zoals in figuur 5. Toevoeging van meerdere
  nodes resulteert dus in meerdere trappen die aan te passen zijn volgens hun parameters in
  het neuraal netwerk. Op deze manier kan men een functie gaan benaderen.
  Merk op dat het beeld van een neuraal netwerk op deze manier altijd in [0, 1] ligt. Dit vormt
  echter geen probleem voor functies die een beeld hebben buiten dit interval, als we werken met
  een compact domein. Een compact domein resulteert dan ook in een compact beeld voor een
  continue functie, wat dan begrensdheid impliceert voor het beeld. Met een begrensd beeld
  kunnen we makkelijk een functie herschalen, zodat deze het compacte domein afbeeldt in
  [0, 1]. Elke continue functie is dus een triviale factor verwijderd van een benaderend neuraal
  netwerk met een laag.
  We zullen nu laten zien dat we effectief een rechthoek op deze manier kunnen benaderen,
  gegeven een rechthoek, met α, β ∈ [0, 1] met α > β:
  h(x) =
  
  α a ≤ x ≤ b
  β elders (9)
  3.5 Universal Approximatie Stelling 16
  Figuur 5: Output van een neuraal netwerk met parameters: a = 100, b0 = 0, b1 = 100,
  w0 = {1, 2}, w1 = {2, −2}, b = −2
  Beschouw ook de verzamelingen van alle neurale netwerken met nul, ´e´en of n hidden layers.
  N N0 = {σ(wx + b) | w, b ∈ R}
  N N1 = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N N0; wi, b ∈ R; k ∈ N}
  N Nn = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N Nn−1; wi, b ∈ R; k ∈ N}
  N N = ∪∞
  k=0 N Nk (10)
  Hieruit volgt ook duidelijk dat de lineaire combinatie van neurale netwerk plus een extra
  factor door de activatie opnieuw een neuraal netwerk is. Dit zal later handig zijn.
  Voor een gegeven rechthoek h, stellen we een neuraal netwerk op uit N N1 met twee nodes
  van waarden s0(x) en s1(x):
  s0(x) = σ(wx + b0) = σ(w(x − a)) (11)
  s1(x) = σ(wx + b1) = σ(w(x − b))
  3.5 Universal Approximatie Stelling 17
  Hierbij nemen we b0 = −wa en respectievelijk b1 = −wb door de eerder vermelde redenen.
  Merk op dat hieruit volgt:
  s1(x) = s0(x − (b − a))
  De factor w speelt dus een rol in het ’hoekig’ maken van onze benadering. Dit zien we
  duidelijk in de afgeleiden van σ(wx + b).
  d
  dxσ(wx + b) = w · σ′(wx + b) (12)
  De sigmoidale functie is begrensd en dus ziet men duidelijk dat een grotere waarde voor w
  zorgt voor een extremere afgeleiden en dus een hoekig karakter. Beschouw nu de voorlopige
  functie f :
  fw(x) = σ(w1s0(x) − w1s1(x) + b) (13)
  We maken deze afhankelijk van een w zoals gedefinieerd bij (11), omdat deze factor uitein-
  delijk onze benadering zal verbeteren als deze naar +∞ gaat. We stellen het tweede gewicht
  hier gelijk aan −w1 zodat de begin- en eindhoogte hetzelfde zal blijven zoals β gedefinieerd
  voor h. Om te bewijzen dat dit een rechthoek wordt, gaan we bewijzen dat deze een afgelei-
  den zal hebben die willekeurig dicht zal liggen bij het volgende schema:
  f ′
  +∞(x) 0 +∞ 0 −∞ 0
  x a b (14)
  Voor we hieraan beginnen, nemen we aan voor de activatie functie dat deze sigmoidal is
  zoals beschreven bij (8). Verder nemen we ook aan dat:
  • σ is continu op R
  • σ′(x) ≥ 0 ∀x ∈ R
  • limx→∞ xσ′(x) = 0
  Uit de limieten van een sigmoidale functie volgt dus dat limx→±∞ σ′(x) = 0. Merk op dat
  dit allemaal geldt voor bijvoorbeeld de sigmoid functie σ(x) = 1/(1 + e−x)
  Voor duidelijkheid defini¨eren we volgende functie g:
  g(x) = w1s0(x) − w1s1(x) + b
  g′(x) = w1 · w(s′
  0(x) − s′
  1(x)) = w1 · wσ′(w(x − a)) − σ′
  3.5 Universal Approximatie Stelling 18
  Er geldt dan, voor σ′(0) = c > 0:
  lim
  w→+∞ w · σ′(w0) = lim
  w→+∞ w · c = +∞
  Als x̸ = 0 dan krijgt men het volgende:
  lim
  w→+∞ w · σ′(wx) = lim
  u→+∞ u · σ′(u)/x = 0/x = 0
  Als we x = a of x = b nemen zien we dus dat g′(x), voor een w naar oneindig, respectievelijk
  naar plus en min oneindig gaat in orde van w zelf. Dit omdat σ′(w · ±(a − b)) sowieso naar
  nul gaat voor w naar oneindig in g′(x) en hebben we dus g′(x) = w1 · ±wσ′(0). Dus kunnen
  we gewoon w · σ′(0) toepassen zoals hierboven beschreven. Voor x̸ = a, b krijgen we dan
  voor g′(x) dat dit gewoon nul min nul is door het hierboven beschreven limiet. Omdat g(a)
  en g(b), voor w naar oneindig, verschillend van nul zijn en niet zal divergeren zal f ′(x) naar
  respectievelijk plus en min oneindig gaan. Voor x̸ = a of x̸ = b geldt dit ook en zal f ′(x)
  naar nul gaan.
  Verder beschouwen we de functiewaarden met het idee we onze parameters te kunnen aan-
  pa
  w→+∞ σ(w1 · σ(w(x − a)) − w1σ(w(x − b)) + b)
  = σ(w1 · 1 − w1 · 1 + b) = σ(b)
  Men ziet dat dit resulteert in een rechthoek met hoogtes σ(b) en σ(w1 +b). Voor de rechthoek
  h(x) gedefinieerd bij (9), is dit dan een kwestie van de vergelijking op te lossen.
  α = σ(w1 + b) (16)
  β = σ(b)
  3.5 Universal Approximatie Stelling 19
  We krijgen dus:
  b = σ−1(β)
  w1 = σ−1(α) − σ−1(β)
  Moest een of twee van de waarden gelijk zijn aan nul of ´e´en kan men de parameters ook
  naar plus of min oneindig laten gaan zoals w dat doet in de functie fw(x), door bijvoorbeeld
  b = −w, w1 = 2w voor een rechthoek met hoogtes nul en ´e´en. Deze laatste zodat w1 groter
  blijft dan b en w1 + b wel degelijk naar +∞ gaat. Uit (16) kan men ook duidelijk zien dat
  moesten we een sigmoidale functie hebben waarvoor geldt:
  lim
  x→∞ σ(x) = c (17)
  dat we dan α = σ(cw1 + b) krijgen en dit dus ook oplosbaar is voor w1 en b te bepalen. De
  redenering is volledig identiek.
  Gegeven dus een stap functie h(x) kan men hier dus een willekeurig dichte benadering van
  geven.
  ∀x, ∀ϵ > 0, ∃w(x, ϵ) > 0 : |fw(x) − h(x)| ≤ ϵ (18)
  Dit concept kan uitgebreid worden tot meerdere hoogtes door toevoeging van meer nodes
  in de hidden layer. De uitwerking is analoog aan deze uitwerking voor meerdere trappen en
  dus meerdere nodes.
  Voor de benadering van een continue functie, splitsen we het interval [0, 1] op in n even
  grote delen [xi, xi+1] voor 0 ≤ i ∈ N ≤ n. Op deze intervallen defini¨eren we een h zodat deze
  c
  = L
  n + ϵw (20)
  Waarbij L uiteraard de maximale Lipschitz constante is, de constante die geldt over het hele
  interval waar we f op benaderen, en x = argmaxx∈[xi,xi+1]f (x) en y = argminx∈[xi,xi+1]f (x).
  De waarde ϵw is dus afhankelijk van de parameter w die naar oneindig gaat. Deze hangt
  af van hoe snel de σ functie naar ´e´en gaat in zijn limiet naar oneindig. Deze is dus functie
  specifiek en laten we open voor het algemene geval.
  Men ziet duidelijk dat de definitie van h niet per se belangrijk is, maar wel goed genoeg moet
  zijn. De waarden voor n, het aantal intervallen te verdelen over [0, 1], is door het karakter
  van het neuraal netwerk ook het aantal nodes in die zijn enige hidden layer. Hieruit volgt
  dan duidelijk:
  ∀ϵ > 0, ∃n0 > L
  ϵ − ϵw
  ∈ N, ∀n ≥ n0 : ||f − h||L2  ϵ
  In literatuur wordt er naar dit theorema verwezen als het feit dat verzameling van functies
  beschreven door neurale netwerken dicht is in de verzameling van continue functies op een
  compacte set. De implicatie die het geeft, is wat er zo belangrijk is voor neurale netwerken:
  alle continue functies kunnen willekeurig dicht benaderd worden door een neuraal netwerk
  op een compacte verzameling.
  3.5 Universal Approximatie Stelling 14
  3.5.1 Sigmoidale functies
  In deze sectie laten we hier een visueel bewijs van zien op basis van [?]. Deze was wel met
  de beperking van sigmoidale functies waarvoor geldt:
  lim
  x→∞ f (x) = 1 lim
  x→−∞ f (x) = 0 (8)
  Als men spreekt over hyperparameters binnen de context van machine learning, bedoelt men
de parameters die gekozen worden voordat het leerproces begonnen is.Dit zijn dan parame-
ters zoals: de topologie van het netwerk, de learning rate, de cost functie, de activatiefuncties
of zelfs convergentie methoden (variaties op gradi¨ent descent). Deze hebben dan ook enkel
invloed op het leerproces, en geen invloed meer op het netwerk zelf. De juiste parameters
kiezen is essentieel om niet alleen het leerproces te versnellen, maar ook de kwaliteit van
het netwerk te verhogen. Het manueel tunen van deze parameters kan nogal wat tijd in
beslag nemen en zal vaak niet het beste of zelfs een goed resultaat opleveren. Daarom zal
in dit hoofdstuk gekeken worden, om met behulp van verscheidene methodes dit proces te
automatiseren en optimaliseren.
Formeel gezien, beschouw Θ de hyperparameterruimte. Dit is het cartesisch product van
de ruimtes waar elke hyperparameter zich in bevindt. Nu zoekt men een θ ∈ Θ zodat
θ = argmin f(ˆθ)
ˆθ∈Θ
. De functie f is hier de loss functie van het neuraal netwerk dat een opti-
male set hyperparameters nodig heeft.
Hier onder zullen verschillende methodes besproken worden, die gebruikt worden om een
maxima te vinden binnen deze hyperparameterruimte.
4.2 Grid search
Grid search is een methode waarbij men itereert over de verschillende hyperparametercom-
binaties binnen (een deel van) de parameterruimte. Vervolgens kijkt men welke hyperpa-
rametercombinatie de laagste cost heeft. Dit algoritme is rekenkundig zeer intensief, maar
het proces is parallelliseerbaar. Het algoritme is,zoals random search, niet erg effici¨ent.
4.3 Random Search
Random search is een alternatieve methode voor hyperparameteroptimalisatie. In tegen-
stelling tot grid search, waarbij alle mogelijke combinaties van hyperparameters worden
uitgeprobeerd, kiest random search willekeurig verschillende combinaties om te evalueren.
Wederom is deze methode enorm parallelliseerbaar aangezien geen enkele uitvoering van
random search afhangt van de volgende of vorige.
4.4 Bayesiaanse Optimalisatie 34
4.4 Bayesiaanse Optimalisatie
Bayesiaanse optimalisatie is een methode om de extreme waarden te vinden van een moeilijk
te evalueren ’black box’ functie f (x), waar traditionele optimalisatiemethodes niet werken.
Het vergt geen kennis van de afgeleide van de hypothetische functie. Er zijn verschillende
varianten op deze methode. Wetende dat de functie moeilijk te evalueren is, werkt het
algoritme op zo weinig mogelijk ge¨evalueerde punten om de extreme waarde op een comp
  Deze eigenschap is zeer belangrijk omdat men hiermee de functie kan vormen tot een trap
  functie (zie figuur 4), die nodig zal zijn in het bewijs. We zullen later aantonen dat dit idee
  uitgebreid kan worden tot functies verschillend van de sigmoidale functies.
  Het idee achter het bewijs is om voor elke functie een benaderende trapfunctie te construeren,
  net zoals de Riemann sommen een integraal benaderen.
  Met de transformatie uit figuur 4 hebben we al een trap kunnen vormen. Om een tweede
  trap te vormen moeten we eerst een afstand cre¨eren. Als referentie punt pakken we, voor
  de sigmoid functie σ(x) = 1
  1+e−x , even het punt waar σ(ax) = 1/2. Dit komt neer op het
  punt waar geldt e−ax = 1 en dus x = 0. Voor σ(ax + b) = 1/2 krijgen we uiteindelijk
  x = − b
  a . We kunnen a en b dus zo veranderen, zodat we een ruimte van grootte b
  a cre¨eren
  tussen functies σ(ax) en σ(ax + b). Of algemener; een afstand van |b1−b0|
  |a| voor σ(ax + b0)
  en σ(ax + b1). Merk op dat dit een lineaire transformatie is binnen de sigmoid functie, net
  zoals de vector berekeningen voor een neuraal netwerk binnen de activatiefunctie. En dus
  de functies s0,1(x) = σ(ax + b0,1) kunnen we beschouwen als de twee eerste hidden nodes, na
  input, met dezelfde weight a en verschillende biases b0, b1. En deze a speelt enkel een rol in
  het meer hoekig maken van de functies.
  3.5 Universal Approximatie Stelling 15
  Figuur 4: De sigmoid functie met verschillende input
  Men kan dan s0 en s1 vermenigvuldigen met een gewenste traphoogte (dit kan door sigmoidale
  activatiefunctie, ze hebben immers hoogte gaande naar 1), w0 en w1 respectievelijk. Optelling
  met elkaar levert dan weer een trapfunctie met een extra trap. Dit weer gestuurd door de
  sigmoid functie als activatie functie, zou de output zijn van een neuraal netwerk met een
  hidden layer die twee nodes (s0 en s1) bevat, zoals in figuur 5. Toevoeging van meerdere
  nodes resulteert dus in meerdere trappen die aan te passen zijn volgens hun parameters in
  het neuraal netwerk. Op deze manier kan men een functie gaan benaderen.
  Merk op dat het beeld van een neuraal netwerk op deze manier altijd in [0, 1] ligt. Dit vormt
  echter geen probleem voor functies die een beeld hebben buiten dit interval, als we werken met
  een compact domein. Een compact domein resulteert dan ook in een compact beeld voor een
  continue functie, wat dan begrensdheid impliceert voor het beeld. Met een begrensd beeld
  kunnen we makkelijk een functie herschalen, zodat deze het compacte domein afbeeldt in
  [0, 1]. Elke continue functie is dus een triviale factor verwijderd van een benaderend neuraal
  netwerk met een laag.
  We zullen nu laten zien dat we effectief een rechthoek op deze manier kunnen benaderen,
  gegeven een rechthoek, met α, β ∈ [0, 1] met α > β:
  h(x) =
  
  α a ≤ x ≤ b
  β elders (9)
  3.5 Universal Approximatie Stelling 16
  Figuur 5: Output van een neuraal netwerk met parameters: a = 100, b0 = 0, b1 = 100,
  w0 = {1, 2}, w1 = {2, −2}, b = −2
  Beschouw ook de verzamelingen van alle neurale netwerken met nul, ´e´en of n hidden layers.
  N N0 = {σ(wx + b) | w, b ∈ R}
  N N1 = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N N0; wi, b ∈ R; k ∈ N}
  N Nn = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N Nn−1; wi, b ∈ R; k ∈ N}
  N N = ∪∞
  k=0 N Nk (10)
  Hieruit volgt ook duidelijk dat de lineaire combinatie van neurale netwerk plus een extra
  factor door de activatie opnieuw een neuraal netwerk is. Dit zal later handig zijn.
  Voor een gegeven rechthoek h, stellen we een neuraal netwerk op uit N N1 met twee nodes
  van waarden s0(x) en s1(x):
  s0(x) = σ(wx + b0) = σ(w(x − a)) (11)
  s1(x) = σ(wx + b1) = σ(w(x − b))
  3.5 Universal Approximatie Stelling 17
  Hierbij nemen we b0 = −wa en respectievelijk b1 = −wb door de eerder vermelde redenen.
  Merk op dat hieruit volgt:
  s1(x) = s0(x − (b − a))
  De factor w speelt dus een rol in het ’hoekig’ maken van onze benadering. Dit zien we
  duidelijk in de afgeleiden van σ(wx + b).
  d
  dxσ(wx + b) = w · σ′(wx + b) (12)
  De sigmoidale functie is begrensd en dus ziet men duidelijk dat een grotere waarde voor w
  zorgt voor een extremere afgeleiden en dus een hoekig karakter. Beschouw nu de voorlopige
  functie f :
  fw(x) = σ(w1s0(x) − w1s1(x) + b) (13)
  We maken deze afhankelijk van een w zoals gedefinieerd bij (11), omdat deze factor uitein-
  delijk onze benadering zal verbeteren als deze naar +∞ gaat. We stellen het tweede gewicht
  hier gelijk aan −w1 zodat de begin- en eindhoogte hetzelfde zal blijven zoals β gedefinieerd
  voor h. Om te bewijzen dat dit een rechthoek wordt, gaan we bewijzen dat deze een afgelei-
  den zal hebben die willekeurig dicht zal liggen bij het volgende schema:
  f ′
  +∞(x) 0 +∞ 0 −∞ 0
  x a b (14)
  Voor we hieraan beginnen, nemen we aan voor de activatie functie dat deze sigmoidal is
  zoals beschreven bij (8). Verder nemen we ook aan dat:
  • σ is continu op R
  • σ′(x) ≥ 0 ∀x ∈ R
  • limx→∞ xσ′(x) = 0
  Uit de limieten van een sigmoidale functie volgt dus dat limx→±∞ σ′(x) = 0. Merk op dat
  dit allemaal geldt voor bijvoorbeeld de sigmoid functie σ(x) = 1/(1 + e−x)
  Voor duidelijkheid defini¨eren we volgende functie g:
  g(x) = w1s0(x) − w1s1(x) + b
  g′(x) = w1 · w(s′
  0(x) − s′
  1(x)) = w1 · wσ′(w(x − a)) − σ′
  3.5 Universal Approximatie Stelling 18
  Er geldt dan, voor σ′(0) = c > 0:
  lim
  w→+∞ w · σ′(w0) = lim
  w→+∞ w · c = +∞
  Als x̸ = 0 dan krijgt men het volgende:
  lim
  w→+∞ w · σ′(wx) = lim
  u→+∞ u · σ′(u)/x = 0/x = 0
  Als we x = a of x = b nemen zien we dus dat g′(x), voor een w naar oneindig, respectievelijk
  naar plus en min oneindig gaat in orde van w zelf. Dit omdat σ′(w · ±(a − b)) sowieso naar
  nul gaat voor w naar oneindig in g′(x) en hebben we dus g′(x) = w1 · ±wσ′(0). Dus kunnen
  we gewoon w · σ′(0) toepassen zoals hierboven beschreven. Voor x̸ = a, b krijgen we dan
  voor g′(x) dat dit gewoon nul min nul is door het hierboven beschreven limiet. Omdat g(a)
  en g(b), voor w naar oneindig, verschillend van nul zijn en niet zal divergeren zal f ′(x) naar
  respectievelijk plus en min oneindig gaan. Voor x̸ = a of x̸ = b geldt dit ook en zal f ′(x)
  naar nul gaan.
  Verder beschouwen we de functiewaarden met het idee we onze parameters te kunnen aan-
  pa
  w→+∞ σ(w1 · σ(w(x − a)) − w1σ(w(x − b)) + b)
  = σ(w1 · 1 − w1 · 1 + b) = σ(b)
  Men ziet dat dit resulteert in een rechthoek met hoogtes σ(b) en σ(w1 +b). Voor de rechthoek
  h(x) gedefinieerd bij (9), is dit dan een kwestie van de vergelijking op te lossen.
  α = σ(w1 + b) (16)
  β = σ(b)
  3.5 Universal Approximatie Stelling 19
  We krijgen dus:
  b = σ−1(β)
  w1 = σ−1(α) − σ−1(β)
  Moest een of twee van de waarden gelijk zijn aan nul of ´e´en kan men de parameters ook
  naar plus of min oneindig laten gaan zoals w dat doet in de functie fw(x), door bijvoorbeeld
  b = −w, w1 = 2w voor een rechthoek met hoogtes nul en ´e´en. Deze laatste zodat w1 groter
  blijft dan b en w1 + b wel degelijk naar +∞ gaat. Uit (16) kan men ook duidelijk zien dat
  moesten we een sigmoidale functie hebben waarvoor geldt:
  lim
  x→∞ σ(x) = c (17)
  dat we dan α = σ(cw1 + b) krijgen en dit dus ook oplosbaar is voor w1 en b te bepalen. De
  redenering is volledig identiek.
  Gegeven dus een stap functie h(x) kan men hier dus een willekeurig dichte benadering van
  geven.
  ∀x, ∀ϵ > 0, ∃w(x, ϵ) > 0 : |fw(x) − h(x)| ≤ ϵ (18)
  Dit concept kan uitgebreid worden tot meerdere hoogtes door toevoeging van meer nodes
  in de hidden layer. De uitwerking is analoog aan deze uitwerking voor meerdere trappen en
  dus meerdere nodes.
  Voor de benadering van een continue functie, splitsen we het interval [0, 1] op in n even
  grote delen [xi, xi+1] voor 0 ≤ i ∈ N ≤ n. Op deze intervallen defini¨eren we een h zodat deze
  c
  = L
  n + ϵw (20)
  Waarbij L uiteraard de maximale Lipschitz constante is, de constante die geldt over het hele
  interval waar we f op benaderen, en x = argmaxx∈[xi,xi+1]f (x) en y = argminx∈[xi,xi+1]f (x).
  De waarde ϵw is dus afhankelijk van de parameter w die naar oneindig gaat. Deze hangt
  af van hoe snel de σ functie naar ´e´en gaat in zijn limiet naar oneindig. Deze is dus functie
  specifiek en laten we open voor het algemene geval.
  Men ziet duidelijk dat de definitie van h niet per se belangrijk is, maar wel goed genoeg moet
  zijn. De waarden voor n, het aantal intervallen te verdelen over [0, 1], is door het karakter
  van het neuraal netwerk ook het aantal nodes in die zijn enige hidden layer. Hieruit volgt
  dan duidelijk:
  ∀ϵ > 0, ∃n0 > L
  ϵ − ϵw
  ∈ N, ∀n ≥ n0 : ||f − h||L2  ϵ
  In literatuur wordt er naar dit theorema verwezen als het feit dat verzameling van functies
  beschreven door neurale netwerken dicht is in de verzameling van continue functies op een
  compacte set. De implicatie die het geeft, is wat er zo belangrijk is voor neurale netwerken:
  alle continue functies kunnen willekeurig dicht benaderd worden door een neuraal netwerk
  op een compacte verzameling.
  3.5 Universal Approximatie Stelling 14
  3.5.1 Sigmoidale functies
  In deze sectie laten we hier een visueel bewijs van zien op basis van [?]. Deze was wel met
  de beperking van sigmoidale functies waarvoor geldt:
  lim
  x→∞ f (x) = 1 lim
  x→−∞ f (x) = 0 (8)
  Als men spreekt over hyperparameters binnen de context van machine learning, bedoelt men
de parameters die gekozen worden voordat het leerproces begonnen is.Dit zijn dan parame-
ters zoals: de topologie van het netwerk, de learning rate, de cost functie, de activatiefuncties
of zelfs convergentie methoden (variaties op gradi¨ent descent). Deze hebben dan ook enkel
invloed op het leerproces, en geen invloed meer op het netwerk zelf. De juiste parameters
kiezen is essentieel om niet alleen het leerproces te versnellen, maar ook de kwaliteit van
het netwerk te verhogen. Het manueel tunen van deze parameters kan nogal wat tijd in
beslag nemen en zal vaak niet het beste of zelfs een goed resultaat opleveren. Daarom zal
in dit hoofdstuk gekeken worden, om met behulp van verscheidene methodes dit proces te
automatiseren en optimaliseren.
Formeel gezien, beschouw Θ de hyperparameterruimte. Dit is het cartesisch product van
de ruimtes waar elke hyperparameter zich in bevindt. Nu zoekt men een θ ∈ Θ zodat
θ = argmin f(ˆθ)
ˆθ∈Θ
. De functie f is hier de loss functie van het neuraal netwerk dat een opti-
male set hyperparameters nodig heeft.
Hier onder zullen verschillende methodes besproken worden, die gebruikt worden om een
maxima te vinden binnen deze hyperparameterruimte.
4.2 Grid search
Grid search is een methode waarbij men itereert over de verschillende hyperparametercom-
binaties binnen (een deel van) de parameterruimte. Vervolgens kijkt men welke hyperpa-
rametercombinatie de laagste cost heeft. Dit algoritme is rekenkundig zeer intensief, maar
het proces is parallelliseerbaar. Het algoritme is,zoals random search, niet erg effici¨ent.
4.3 Random Search
Random search is een alternatieve methode voor hyperparameteroptimalisatie. In tegen-
stelling tot grid search, waarbij alle mogelijke combinaties van hyperparameters worden
uitgeprobeerd, kiest random search willekeurig verschillende combinaties om te evalueren.
Wederom is deze methode enorm parallelliseerbaar aangezien geen enkele uitvoering van
random search afhangt van de volgende of vorige.
4.4 Bayesiaanse Optimalisatie 34
4.4 Bayesiaanse Optimalisatie
Bayesiaanse optimalisatie is een methode om de extreme waarden te vinden van een moeilijk
te evalueren ’black box’ functie f (x), waar traditionele optimalisatiemethodes niet werken.
Het vergt geen kennis van de afgeleide van de hypothetische functie. Er zijn verschillende
varianten op deze methode. Wetende dat de functie moeilijk te evalueren is, werkt het
algoritme op zo weinig mogelijk ge¨evalueerde punten om de extreme waarde op een comp
  Deze eigenschap is zeer belangrijk omdat men hiermee de functie kan vormen tot een trap
  functie (zie figuur 4), die nodig zal zijn in het bewijs. We zullen later aantonen dat dit idee
  uitgebreid kan worden tot functies verschillend van de sigmoidale functies.
  Het idee achter het bewijs is om voor elke functie een benaderende trapfunctie te construeren,
  net zoals de Riemann sommen een integraal benaderen.
  Met de transformatie uit figuur 4 hebben we al een trap kunnen vormen. Om een tweede
  trap te vormen moeten we eerst een afstand cre¨eren. Als referentie punt pakken we, voor
  de sigmoid functie σ(x) = 1
  1+e−x , even het punt waar σ(ax) = 1/2. Dit komt neer op het
  punt waar geldt e−ax = 1 en dus x = 0. Voor σ(ax + b) = 1/2 krijgen we uiteindelijk
  x = − b
  a . We kunnen a en b dus zo veranderen, zodat we een ruimte van grootte b
  a cre¨eren
  tussen functies σ(ax) en σ(ax + b). Of algemener; een afstand van |b1−b0|
  |a| voor σ(ax + b0)
  en σ(ax + b1). Merk op dat dit een lineaire transformatie is binnen de sigmoid functie, net
  zoals de vector berekeningen voor een neuraal netwerk binnen de activatiefunctie. En dus
  de functies s0,1(x) = σ(ax + b0,1) kunnen we beschouwen als de twee eerste hidden nodes, na
  input, met dezelfde weight a en verschillende biases b0, b1. En deze a speelt enkel een rol in
  het meer hoekig maken van de functies.
  3.5 Universal Approximatie Stelling 15
  Figuur 4: De sigmoid functie met verschillende input
  Men kan dan s0 en s1 vermenigvuldigen met een gewenste traphoogte (dit kan door sigmoidale
  activatiefunctie, ze hebben immers hoogte gaande naar 1), w0 en w1 respectievelijk. Optelling
  met elkaar levert dan weer een trapfunctie met een extra trap. Dit weer gestuurd door de
  sigmoid functie als activatie functie, zou de output zijn van een neuraal netwerk met een
  hidden layer die twee nodes (s0 en s1) bevat, zoals in figuur 5. Toevoeging van meerdere
  nodes resulteert dus in meerdere trappen die aan te passen zijn volgens hun parameters in
  het neuraal netwerk. Op deze manier kan men een functie gaan benaderen.
  Merk op dat het beeld van een neuraal netwerk op deze manier altijd in [0, 1] ligt. Dit vormt
  echter geen probleem voor functies die een beeld hebben buiten dit interval, als we werken met
  een compact domein. Een compact domein resulteert dan ook in een compact beeld voor een
  continue functie, wat dan begrensdheid impliceert voor het beeld. Met een begrensd beeld
  kunnen we makkelijk een functie herschalen, zodat deze het compacte domein afbeeldt in
  [0, 1]. Elke continue functie is dus een triviale factor verwijderd van een benaderend neuraal
  netwerk met een laag.
  We zullen nu laten zien dat we effectief een rechthoek op deze manier kunnen benaderen,
  gegeven een rechthoek, met α, β ∈ [0, 1] met α > β:
  h(x) =
  
  α a ≤ x ≤ b
  β elders (9)
  3.5 Universal Approximatie Stelling 16
  Figuur 5: Output van een neuraal netwerk met parameters: a = 100, b0 = 0, b1 = 100,
  w0 = {1, 2}, w1 = {2, −2}, b = −2
  Beschouw ook de verzamelingen van alle neurale netwerken met nul, ´e´en of n hidden layers.
  N N0 = {σ(wx + b) | w, b ∈ R}
  N N1 = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N N0; wi, b ∈ R; k ∈ N}
  N Nn = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N Nn−1; wi, b ∈ R; k ∈ N}
  N N = ∪∞
  k=0 N Nk (10)
  Hieruit volgt ook duidelijk dat de lineaire combinatie van neurale netwerk plus een extra
  factor door de activatie opnieuw een neuraal netwerk is. Dit zal later handig zijn.
  Voor een gegeven rechthoek h, stellen we een neuraal netwerk op uit N N1 met twee nodes
  van waarden s0(x) en s1(x):
  s0(x) = σ(wx + b0) = σ(w(x − a)) (11)
  s1(x) = σ(wx + b1) = σ(w(x − b))
  3.5 Universal Approximatie Stelling 17
  Hierbij nemen we b0 = −wa en respectievelijk b1 = −wb door de eerder vermelde redenen.
  Merk op dat hieruit volgt:
  s1(x) = s0(x − (b − a))
  De factor w speelt dus een rol in het ’hoekig’ maken van onze benadering. Dit zien we
  duidelijk in de afgeleiden van σ(wx + b).
  d
  dxσ(wx + b) = w · σ′(wx + b) (12)
  De sigmoidale functie is begrensd en dus ziet men duidelijk dat een grotere waarde voor w
  zorgt voor een extremere afgeleiden en dus een hoekig karakter. Beschouw nu de voorlopige
  functie f :
  fw(x) = σ(w1s0(x) − w1s1(x) + b) (13)
  We maken deze afhankelijk van een w zoals gedefinieerd bij (11), omdat deze factor uitein-
  delijk onze benadering zal verbeteren als deze naar +∞ gaat. We stellen het tweede gewicht
  hier gelijk aan −w1 zodat de begin- en eindhoogte hetzelfde zal blijven zoals β gedefinieerd
  voor h. Om te bewijzen dat dit een rechthoek wordt, gaan we bewijzen dat deze een afgelei-
  den zal hebben die willekeurig dicht zal liggen bij het volgende schema:
  f ′
  +∞(x) 0 +∞ 0 −∞ 0
  x a b (14)
  Voor we hieraan beginnen, nemen we aan voor de activatie functie dat deze sigmoidal is
  zoals beschreven bij (8). Verder nemen we ook aan dat:
  • σ is continu op R
  • σ′(x) ≥ 0 ∀x ∈ R
  • limx→∞ xσ′(x) = 0
  Uit de limieten van een sigmoidale functie volgt dus dat limx→±∞ σ′(x) = 0. Merk op dat
  dit allemaal geldt voor bijvoorbeeld de sigmoid functie σ(x) = 1/(1 + e−x)
  Voor duidelijkheid defini¨eren we volgende functie g:
  g(x) = w1s0(x) − w1s1(x) + b
  g′(x) = w1 · w(s′
  0(x) − s′
  1(x)) = w1 · wσ′(w(x − a)) − σ′
  3.5 Universal Approximatie Stelling 18
  Er geldt dan, voor σ′(0) = c > 0:
  lim
  w→+∞ w · σ′(w0) = lim
  w→+∞ w · c = +∞
  Als x̸ = 0 dan krijgt men het volgende:
  lim
  w→+∞ w · σ′(wx) = lim
  u→+∞ u · σ′(u)/x = 0/x = 0
  Als we x = a of x = b nemen zien we dus dat g′(x), voor een w naar oneindig, respectievelijk
  naar plus en min oneindig gaat in orde van w zelf. Dit omdat σ′(w · ±(a − b)) sowieso naar
  nul gaat voor w naar oneindig in g′(x) en hebben we dus g′(x) = w1 · ±wσ′(0). Dus kunnen
  we gewoon w · σ′(0) toepassen zoals hierboven beschreven. Voor x̸ = a, b krijgen we dan
  voor g′(x) dat dit gewoon nul min nul is door het hierboven beschreven limiet. Omdat g(a)
  en g(b), voor w naar oneindig, verschillend van nul zijn en niet zal divergeren zal f ′(x) naar
  respectievelijk plus en min oneindig gaan. Voor x̸ = a of x̸ = b geldt dit ook en zal f ′(x)
  naar nul gaan.
  Verder beschouwen we de functiewaarden met het idee we onze parameters te kunnen aan-
  pa
  w→+∞ σ(w1 · σ(w(x − a)) − w1σ(w(x − b)) + b)
  = σ(w1 · 1 − w1 · 1 + b) = σ(b)
  Men ziet dat dit resulteert in een rechthoek met hoogtes σ(b) en σ(w1 +b). Voor de rechthoek
  h(x) gedefinieerd bij (9), is dit dan een kwestie van de vergelijking op te lossen.
  α = σ(w1 + b) (16)
  β = σ(b)
  3.5 Universal Approximatie Stelling 19
  We krijgen dus:
  b = σ−1(β)
  w1 = σ−1(α) − σ−1(β)
  Moest een of twee van de waarden gelijk zijn aan nul of ´e´en kan men de parameters ook
  naar plus of min oneindig laten gaan zoals w dat doet in de functie fw(x), door bijvoorbeeld
  b = −w, w1 = 2w voor een rechthoek met hoogtes nul en ´e´en. Deze laatste zodat w1 groter
  blijft dan b en w1 + b wel degelijk naar +∞ gaat. Uit (16) kan men ook duidelijk zien dat
  moesten we een sigmoidale functie hebben waarvoor geldt:
  lim
  x→∞ σ(x) = c (17)
  dat we dan α = σ(cw1 + b) krijgen en dit dus ook oplosbaar is voor w1 en b te bepalen. De
  redenering is volledig identiek.
  Gegeven dus een stap functie h(x) kan men hier dus een willekeurig dichte benadering van
  geven.
  ∀x, ∀ϵ > 0, ∃w(x, ϵ) > 0 : |fw(x) − h(x)| ≤ ϵ (18)
  Dit concept kan uitgebreid worden tot meerdere hoogtes door toevoeging van meer nodes
  in de hidden layer. De uitwerking is analoog aan deze uitwerking voor meerdere trappen en
  dus meerdere nodes.
  Voor de benadering van een continue functie, splitsen we het interval [0, 1] op in n even
  grote delen [xi, xi+1] voor 0 ≤ i ∈ N ≤ n. Op deze intervallen defini¨eren we een h zodat deze
  c
  = L
  n + ϵw (20)
  Waarbij L uiteraard de maximale Lipschitz constante is, de constante die geldt over het hele
  interval waar we f op benaderen, en x = argmaxx∈[xi,xi+1]f (x) en y = argminx∈[xi,xi+1]f (x).
  De waarde ϵw is dus afhankelijk van de parameter w die naar oneindig gaat. Deze hangt
  af van hoe snel de σ functie naar ´e´en gaat in zijn limiet naar oneindig. Deze is dus functie
  specifiek en laten we open voor het algemene geval.
  Men ziet duidelijk dat de definitie van h niet per se belangrijk is, maar wel goed genoeg moet
  zijn. De waarden voor n, het aantal intervallen te verdelen over [0, 1], is door het karakter
  van het neuraal netwerk ook het aantal nodes in die zijn enige hidden layer. Hieruit volgt
  dan duidelijk:
  ∀ϵ > 0, ∃n0 > L
  ϵ − ϵw
  ∈ N, ∀n ≥ n0 : ||f − h||L2  ϵIn literatuur wordt er naar dit theorema verwezen als het feit dat verzameling van functies
  beschreven door neurale netwerken dicht is in de verzameling van continue functies op een
  compacte set. De implicatie die het geeft, is wat er zo belangrijk is voor neurale netwerken:
  alle continue functies kunnen willekeurig dicht benaderd worden door een neuraal netwerk
  op een compacte verzameling.
  3.5 Universal Approximatie Stelling 14
  3.5.1 Sigmoidale functies
  In deze sectie laten we hier een visueel bewijs van zien op basis van [?]. Deze was wel met
  de beperking van sigmoidale functies waarvoor geldt:
  lim
  x→∞ f (x) = 1 lim
  x→−∞ f (x) = 0 (8)
  Als men spreekt over hyperparameters binnen de context van machine learning, bedoelt men
de parameters die gekozen worden voordat het leerproces begonnen is.Dit zijn dan parame-
ters zoals: de topologie van het netwerk, de learning rate, de cost functie, de activatiefuncties
of zelfs convergentie methoden (variaties op gradi¨ent descent). Deze hebben dan ook enkel
invloed op het leerproces, en geen invloed meer op het netwerk zelf. De juiste parameters
kiezen is essentieel om niet alleen het leerproces te versnellen, maar ook de kwaliteit van
het netwerk te verhogen. Het manueel tunen van deze parameters kan nogal wat tijd in
beslag nemen en zal vaak niet het beste of zelfs een goed resultaat opleveren. Daarom zal
in dit hoofdstuk gekeken worden, om met behulp van verscheidene methodes dit proces te
automatiseren en optimaliseren.
Formeel gezien, beschouw Θ de hyperparameterruimte. Dit is het cartesisch product van
de ruimtes waar elke hyperparameter zich in bevindt. Nu zoekt men een θ ∈ Θ zodat
θ = argmin f(ˆθ)
ˆθ∈Θ
. De functie f is hier de loss functie van het neuraal netwerk dat een opti-
male set hyperparameters nodig heeft.
Hier onder zullen verschillende methodes besproken worden, die gebruikt worden om een
maxima te vinden binnen deze hyperparameterruimte.
4.2 Grid search
Grid search is een methode waarbij men itereert over de verschillende hyperparametercom-
binaties binnen (een deel van) de parameterruimte. Vervolgens kijkt men welke hyperpa-
rametercombinatie de laagste cost heeft. Dit algoritme is rekenkundig zeer intensief, maar
het proces is parallelliseerbaar. Het algoritme is,zoals random search, niet erg effici¨ent.
4.3 Random Search
Random search is een alternatieve methode voor hyperparameteroptimalisatie. In tegen-
stelling tot grid search, waarbij alle mogelijke combinaties van hyperparameters worden
uitgeprobeerd, kiest random search willekeurig verschillende combinaties om te evalueren.
Wederom is deze methode enorm parallelliseerbaar aangezien geen enkele uitvoering van
random search afhangt van de volgende of vorige.
4.4 Bayesiaanse Optimalisatie 34
4.4 Bayesiaanse Optimalisatie
Bayesiaanse optimalisatie is een methode om de extreme waarden te vinden van een moeilijk
te evalueren ’black box’ functie f (x), waar traditionele optimalisatiemethodes niet werken.
Het vergt geen kennis van de afgeleide van de hypothetische functie. Er zijn verschillende
varianten op deze methode. Wetende dat de functie moeilijk te evalueren is, werkt het
algoritme op zo weinig mogelijk ge¨evalueerde punten om de extreme waarde op een comp
  Deze eigenschap is zeer belangrijk omdat men hiermee de functie kan vormen tot een trap
  functie (zie figuur 4), die nodig zal zijn in het bewijs. We zullen later aantonen dat dit idee
  uitgebreid kan worden tot functies verschillend van de sigmoidale functies.
  Het idee achter het bewijs is om voor elke functie een benaderende trapfunctie te construeren,
  net zoals de Riemann sommen een integraal benaderen.
  Met de transformatie uit figuur 4 hebben we al een trap kunnen vormen. Om een tweede
  trap te vormen moeten we eerst een afstand cre¨eren. Als referentie punt pakken we, voor
  de sigmoid functie σ(x) = 1
  1+e−x , even het punt waar σ(ax) = 1/2. Dit komt neer op het
  punt waar geldt e−ax = 1 en dus x = 0. Voor σ(ax + b) = 1/2 krijgen we uiteindelijk
  x = − b
  a . We kunnen a en b dus zo veranderen, zodat we een ruimte van grootte b
  a cre¨eren
  tussen functies σ(ax) en σ(ax + b). Of algemener; een afstand van |b1−b0|
  |a| voor σ(ax + b0)
  en σ(ax + b1). Merk op dat dit een lineaire transformatie is binnen de sigmoid functie, net
  zoals de vector berekeningen voor een neuraal netwerk binnen de activatiefunctie. En dus
  de functies s0,1(x) = σ(ax + b0,1) kunnen we beschouwen als de twee eerste hidden nodes, na
  input, met dezelfde weight a en verschillende biases b0, b1. En deze a speelt enkel een rol in
  het meer hoekig maken van de functies.
  3.5 Universal Approximatie Stelling 15
  Figuur 4: De sigmoid functie met verschillende input
  Men kan dan s0 en s1 vermenigvuldigen met een gewenste traphoogte (dit kan door sigmoidale
  activatiefunctie, ze hebben immers hoogte gaande naar 1), w0 en w1 respectievelijk. Optelling
  met elkaar levert dan weer een trapfunctie met een extra trap. Dit weer gestuurd door de
  sigmoid functie als activatie functie, zou de output zijn van een neuraal netwerk met een
  hidden layer die twee nodes (s0 en s1) bevat, zoals in figuur 5. Toevoeging van meerdere
  nodes resulteert dus in meerdere trappen die aan te passen zijn volgens hun parameters in
  het neuraal netwerk. Op deze manier kan men een functie gaan benaderen.
  Merk op dat het beeld van een neuraal netwerk op deze manier altijd in [0, 1] ligt. Dit vormt
  echter geen probleem voor functies die een beeld hebben buiten dit interval, als we werken met
  een compact domein. Een compact domein resulteert dan ook in een compact beeld voor een
  continue functie, wat dan begrensdheid impliceert voor het beeld. Met een begrensd beeld
  kunnen we makkelijk een functie herschalen, zodat deze het compacte domein afbeeldt in
  [0, 1]. Elke continue functie is dus een triviale factor verwijderd van een benaderend neuraal
  netwerk met een laag.
  We zullen nu laten zien dat we effectief een rechthoek op deze manier kunnen benaderen,
  gegeven een rechthoek, met α, β ∈ [0, 1] met α > β:
  h(x) =
  
  α a ≤ x ≤ b
  β elders (9)
  3.5 Universal Approximatie Stelling 16
  Figuur 5: Output van een neuraal netwerk met parameters: a = 100, b0 = 0, b1 = 100,
  w0 = {1, 2}, w1 = {2, −2}, b = −2
  Beschouw ook de verzamelingen van alle neurale netwerken met nul, ´e´en of n hidden layers.
  N N0 = {σ(wx + b) | w, b ∈ R}
  N N1 = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N N0; wi, b ∈ R; k ∈ N}
  N Nn = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N Nn−1; wi, b ∈ R; k ∈ N}
  N N = ∪∞
  k=0 N Nk (10)
  Hieruit volgt ook duidelijk dat de lineaire combinatie van neurale netwerk plus een extra
  factor door de activatie opnieuw een neuraal netwerk is. Dit zal later handig zijn.
  Voor een gegeven rechthoek h, stellen we een neuraal netwerk op uit N N1 met twee nodes
  van waarden s0(x) en s1(x):
  s0(x) = σ(wx + b0) = σ(w(x − a)) (11)
  s1(x) = σ(wx + b1) = σ(w(x − b))
  3.5 Universal Approximatie Stelling 17
  Hierbij nemen we b0 = −wa en respectievelijk b1 = −wb door de eerder vermelde redenen.
  Merk op dat hieruit volgt:
  s1(x) = s0(x − (b − a))
  De factor w speelt dus een rol in het ’hoekig’ maken van onze benadering. Dit zien we
  duidelijk in de afgeleiden van σ(wx + b).
  d
  dxσ(wx + b) = w · σ′(wx + b) (12)
  De sigmoidale functie is begrensd en dus ziet men duidelijk dat een grotere waarde voor w
  zorgt voor een extremere afgeleiden en dus een hoekig karakter. Beschouw nu de voorlopige
  functie f :
  fw(x) = σ(w1s0(x) − w1s1(x) + b) (13)
  We maken deze afhankelijk van een w zoals gedefinieerd bij (11), omdat deze factor uitein-
  delijk onze benadering zal verbeteren als deze naar +∞ gaat. We stellen het tweede gewicht
  hier gelijk aan −w1 zodat de begin- en eindhoogte hetzelfde zal blijven zoals β gedefinieerd
  voor h. Om te bewijzen dat dit een rechthoek wordt, gaan we bewijzen dat deze een afgelei-
  den zal hebben die willekeurig dicht zal liggen bij het volgende schema:
  f ′
  +∞(x) 0 +∞ 0 −∞ 0
  x a b (14)
  Voor we hieraan beginnen, nemen we aan voor de activatie functie dat deze sigmoidal is
  zoals beschreven bij (8). Verder nemen we ook aan dat:
  • σ is continu op R
  • σ′(x) ≥ 0 ∀x ∈ R
  • limx→∞ xσ′(x) = 0
  Uit de limieten van een sigmoidale functie volgt dus dat limx→±∞ σ′(x) = 0. Merk op dat
  dit allemaal geldt voor bijvoorbeeld de sigmoid functie σ(x) = 1/(1 + e−x)
  Voor duidelijkheid defini¨eren we volgende functie g:
  g(x) = w1s0(x) − w1s1(x) + b
  g′(x) = w1 · w(s′
  0(x) − s′
  1(x)) = w1 · wσ′(w(x − a)) − σ′
  3.5 Universal Approximatie Stelling 18
  Er geldt dan, voor σ′(0) = c > 0:
  lim
  w→+∞ w · σ′(w0) = lim
  w→+∞ w · c = +∞
  Als x̸ = 0 dan krijgt men het volgende:
  lim
  w→+∞ w · σ′(wx) = lim
  u→+∞ u · σ′(u)/x = 0/x = 0
  Als we x = a of x = b nemen zien we dus dat g′(x), voor een w naar oneindig, respectievelijk
  naar plus en min oneindig gaat in orde van w zelf. Dit omdat σ′(w · ±(a − b)) sowieso naar
  nul gaat voor w naar oneindig in g′(x) en hebben we dus g′(x) = w1 · ±wσ′(0). Dus kunnen
  we gewoon w · σ′(0) toepassen zoals hierboven beschreven. Voor x̸ = a, b krijgen we dan
  voor g′(x) dat dit gewoon nul min nul is door het hierboven beschreven limiet. Omdat g(a)
  en g(b), voor w naar oneindig, verschillend van nul zijn en niet zal divergeren zal f ′(x) naar
  respectievelijk plus en min oneindig gaan. Voor x̸ = a of x̸ = b geldt dit ook en zal f ′(x)
  naar nul gaan.
  Verder beschouwen we de functiewaarden met het idee we onze parameters te kunnen aan-
  pa
  w→+∞ σ(w1 · σ(w(x − a)) − w1σ(w(x − b)) + b)
  = σ(w1 · 1 − w1 · 1 + b) = σ(b)
  Men ziet dat dit resulteert in een rechthoek met hoogtes σ(b) en σ(w1 +b). Voor de rechthoek
  h(x) gedefinieerd bij (9), is dit dan een kwestie van de vergelijking op te lossen.
  α = σ(w1 + b) (16)
  β = σ(b)
  3.5 Universal Approximatie Stelling 19
  We krijgen dus:
  b = σ−1(β)
  w1 = σ−1(α) − σ−1(β)
  Moest een of twee van de waarden gelijk zijn aan nul of ´e´en kan men de parameters ook
  naar plus of min oneindig laten gaan zoals w dat doet in de functie fw(x), door bijvoorbeeld
  b = −w, w1 = 2w voor een rechthoek met hoogtes nul en ´e´en. Deze laatste zodat w1 groter
  blijft dan b en w1 + b wel degelijk naar +∞ gaat. Uit (16) kan men ook duidelijk zien dat
  moesten we een sigmoidale functie hebben waarvoor geldt:
  lim
  x→∞ σ(x) = c (17)
  dat we dan α = σ(cw1 + b) krijgen en dit dus ook oplosbaar is voor w1 en b te bepalen. De
  redenering is volledig identiek.
  Gegeven dus een stap functie h(x) kan men hier dus een willekeurig dichte benadering van
  geven.
  ∀x, ∀ϵ > 0, ∃w(x, ϵ) > 0 : |fw(x) − h(x)| ≤ ϵ (18)
  Dit concept kan uitgebreid worden tot meerdere hoogtes door toevoeging van meer nodes
  in de hidden layer. De uitwerking is analoog aan deze uitwerking voor meerdere trappen en
  dus meerdere nodes.
  Voor de benadering van een continue functie, splitsen we het interval [0, 1] op in n even
  grote delen [xi, xi+1] voor 0 ≤ i ∈ N ≤ n. Op deze intervallen defini¨eren we een h zodat deze
  c
  = L
  n + ϵw (20)
  Waarbij L uiteraard de maximale Lipschitz constante is, de constante die geldt over het hele
  interval waar we f op benaderen, en x = argmaxx∈[xi,xi+1]f (x) en y = argminx∈[xi,xi+1]f (x).
  De waarde ϵw is dus afhankelijk van de parameter w die naar oneindig gaat. Deze hangt
  af van hoe snel de σ functie naar ´e´en gaat in zijn limiet naar oneindig. Deze is dus functie
  specifiek en laten we open voor het algemene geval.
  Men ziet duidelijk dat de definitie van h niet per se belangrijk is, maar wel goed genoeg moet
  zijn. De waarden voor n, het aantal intervallen te verdelen over [0, 1], is door het karakter
  van het neuraal netwerk ook het aantal nodes in die zijn enige hidden layer. Hieruit volgt
  dan duidelijk:
  ∀ϵ > 0, ∃n0 > L
  ϵ − ϵw
  ∈ N, ∀n ≥ n0 : ||f − h||L2  ϵIn literatuur wordt er naar dit theorema verwezen als het feit dat verzameling van functies
  beschreven door neurale netwerken dicht is in de verzameling van continue functies op een
  compacte set. De implicatie die het geeft, is wat er zo belangrijk is voor neurale netwerken:
  alle continue functies kunnen willekeurig dicht benaderd worden door een neuraal netwerk
  op een compacte verzameling.
  3.5 Universal Approximatie Stelling 14
  3.5.1 Sigmoidale functies
  In deze sectie laten we hier een visueel bewijs van zien op basis van [?]. Deze was wel met
  de beperking van sigmoidale functies waarvoor geldt:
  lim
  x→∞ f (x) = 1 lim
  x→−∞ f (x) = 0 (8)
  Als men spreekt over hyperparameters binnen de context van machine learning, bedoelt men
de parameters die gekozen worden voordat het leerproces begonnen is.Dit zijn dan parame-
ters zoals: de topologie van het netwerk, de learning rate, de cost functie, de activatiefuncties
of zelfs convergentie methoden (variaties op gradi¨ent descent). Deze hebben dan ook enkel
invloed op het leerproces, en geen invloed meer op het netwerk zelf. De juiste parameters
kiezen is essentieel om niet alleen het leerproces te versnellen, maar ook de kwaliteit van
het netwerk te verhogen. Het manueel tunen van deze parameters kan nogal wat tijd in
beslag nemen en zal vaak niet het beste of zelfs een goed resultaat opleveren. Daarom zal
in dit hoofdstuk gekeken worden, om met behulp van verscheidene methodes dit proces te
automatiseren en optimaliseren.
Formeel gezien, beschouw Θ de hyperparameterruimte. Dit is het cartesisch product van
de ruimtes waar elke hyperparameter zich in bevindt. Nu zoekt men een θ ∈ Θ zodat
θ = argmin f(ˆθ)
ˆθ∈Θ
. De functie f is hier de loss functie van het neuraal netwerk dat een opti-
male set hyperparameters nodig heeft.
Hier onder zullen verschillende methodes besproken worden, die gebruikt worden om een
maxima te vinden binnen deze hyperparameterruimte.
4.2 Grid search
Grid search is een methode waarbij men itereert over de verschillende hyperparametercom-
binaties binnen (een deel van) de parameterruimte. Vervolgens kijkt men welke hyperpa-
rametercombinatie de laagste cost heeft. Dit algoritme is rekenkundig zeer intensief, maar
het proces is parallelliseerbaar. Het algoritme is,zoals random search, niet erg effici¨ent.
4.3 Random Search
Random search is een alternatieve methode voor hyperparameteroptimalisatie. In tegen-
stelling tot grid search, waarbij alle mogelijke combinaties van hyperparameters worden
uitgeprobeerd, kiest random search willekeurig verschillende combinaties om te evalueren.
Wederom is deze methode enorm parallelliseerbaar aangezien geen enkele uitvoering van
random search afhangt van de volgende of vorige.
4.4 Bayesiaanse Optimalisatie 34
4.4 Bayesiaanse Optimalisatie
Bayesiaanse optimalisatie is een methode om de extreme waarden te vinden van een moeilijk
te evalueren ’black box’ functie f (x), waar traditionele optimalisatiemethodes niet werken.
Het vergt geen kennis van de afgeleide van de hypothetische functie. Er zijn verschillende
varianten op deze methode. Wetende dat de functie moeilijk te evalueren is, werkt het
algoritme op zo weinig mogelijk ge¨evalueerde punten om de extreme waarde op een comp
  Deze eigenschap is zeer belangrijk omdat men hiermee de functie kan vormen tot een trap
  functie (zie figuur 4), die nodig zal zijn in het bewijs. We zullen later aantonen dat dit idee
  uitgebreid kan worden tot functies verschillend van de sigmoidale functies.
  Het idee achter het bewijs is om voor elke functie een benaderende trapfunctie te construeren,
  net zoals de Riemann sommen een integraal benaderen.
  Met de transformatie uit figuur 4 hebben we al een trap kunnen vormen. Om een tweede
  trap te vormen moeten we eerst een afstand cre¨eren. Als referentie punt pakken we, voor
  de sigmoid functie σ(x) = 1
  1+e−x , even het punt waar σ(ax) = 1/2. Dit komt neer op het
  punt waar geldt e−ax = 1 en dus x = 0. Voor σ(ax + b) = 1/2 krijgen we uiteindelijk
  x = − b
  a . We kunnen a en b dus zo veranderen, zodat we een ruimte van grootte b
  a cre¨eren
  tussen functies σ(ax) en σ(ax + b). Of algemener; een afstand van |b1−b0|
  |a| voor σ(ax + b0)
  en σ(ax + b1). Merk op dat dit een lineaire transformatie is binnen de sigmoid functie, net
  zoals de vector berekeningen voor een neuraal netwerk binnen de activatiefunctie. En dus
  de functies s0,1(x) = σ(ax + b0,1) kunnen we beschouwen als de twee eerste hidden nodes, na
  input, met dezelfde weight a en verschillende biases b0, b1. En deze a speelt enkel een rol in
  het meer hoekig maken van de functies.
  3.5 Universal Approximatie Stelling 15
  Figuur 4: De sigmoid functie met verschillende input
  Men kan dan s0 en s1 vermenigvuldigen met een gewenste traphoogte (dit kan door sigmoidale
  activatiefunctie, ze hebben immers hoogte gaande naar 1), w0 en w1 respectievelijk. Optelling
  met elkaar levert dan weer een trapfunctie met een extra trap. Dit weer gestuurd door de
  sigmoid functie als activatie functie, zou de output zijn van een neuraal netwerk met een
  hidden layer die twee nodes (s0 en s1) bevat, zoals in figuur 5. Toevoeging van meerdere
  nodes resulteert dus in meerdere trappen die aan te passen zijn volgens hun parameters in
  het neuraal netwerk. Op deze manier kan men een functie gaan benaderen.
  Merk op dat het beeld van een neuraal netwerk op deze manier altijd in [0, 1] ligt. Dit vormt
  echter geen probleem voor functies die een beeld hebben buiten dit interval, als we werken met
  een compact domein. Een compact domein resulteert dan ook in een compact beeld voor een
  continue functie, wat dan begrensdheid impliceert voor het beeld. Met een begrensd beeld
  kunnen we makkelijk een functie herschalen, zodat deze het compacte domein afbeeldt in
  [0, 1]. Elke continue functie is dus een triviale factor verwijderd van een benaderend neuraal
  netwerk met een laag.
  We zullen nu laten zien dat we effectief een rechthoek op deze manier kunnen benaderen,
  gegeven een rechthoek, met α, β ∈ [0, 1] met α > β:
  h(x) =
  
  α a ≤ x ≤ b
  β elders (9)
  3.5 Universal Approximatie Stelling 16
  Figuur 5: Output van een neuraal netwerk met parameters: a = 100, b0 = 0, b1 = 100,
  w0 = {1, 2}, w1 = {2, −2}, b = −2
  Beschouw ook de verzamelingen van alle neurale netwerken met nul, ´e´en of n hidden layers.
  N N0 = {σ(wx + b) | w, b ∈ R}
  N N1 = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N N0; wi, b ∈ R; k ∈ N}
  N Nn = {σ(
  kX
  i=0
  (wisi(x)) + b) | si ∈ N Nn−1; wi, b ∈ R; k ∈ N}
  N N = ∪∞
  k=0 N Nk (10)
  Hieruit volgt ook duidelijk dat de lineaire combinatie van neurale netwerk plus een extra
  factor door de activatie opnieuw een neuraal netwerk is. Dit zal later handig zijn.
  Voor een gegeven rechthoek h, stellen we een neuraal netwerk op uit N N1 met twee nodes
  van waarden s0(x) en s1(x):
  s0(x) = σ(wx + b0) = σ(w(x − a)) (11)
  s1(x) = σ(wx + b1) = σ(w(x − b))
  3.5 Universal Approximatie Stelling 17
  Hierbij nemen we b0 = −wa en respectievelijk b1 = −wb door de eerder vermelde redenen.
  Merk op dat hieruit volgt:
  s1(x) = s0(x − (b − a))
  De factor w speelt dus een rol in het ’hoekig’ maken van onze benadering. Dit zien we
  duidelijk in de afgeleiden van σ(wx + b).
  d
  dxσ(wx + b) = w · σ′(wx + b) (12)
  De sigmoidale functie is begrensd en dus ziet men duidelijk dat een grotere waarde voor w
  zorgt voor een extremere afgeleiden en dus een hoekig karakter. Beschouw nu de voorlopige
  functie f :
  fw(x) = σ(w1s0(x) − w1s1(x) + b) (13)
  We maken deze afhankelijk van een w zoals gedefinieerd bij (11), omdat deze factor uitein-
  delijk onze benadering zal verbeteren als deze naar +∞ gaat. We stellen het tweede gewicht
  hier gelijk aan −w1 zodat de begin- en eindhoogte hetzelfde zal blijven zoals β gedefinieerd
  voor h. Om te bewijzen dat dit een rechthoek wordt, gaan we bewijzen dat deze een afgelei-
  den zal hebben die willekeurig dicht zal liggen bij het volgende schema:
  f ′
  +∞(x) 0 +∞ 0 −∞ 0
  x a b (14)
  Voor we hieraan beginnen, nemen we aan voor de activatie functie dat deze sigmoidal is
  zoals beschreven bij (8). Verder nemen we ook aan dat:
  • σ is continu op R
  • σ′(x) ≥ 0 ∀x ∈ R
  • limx→∞ xσ′(x) = 0
  Uit de limieten van een sigmoidale functie volgt dus dat limx→±∞ σ′(x) = 0. Merk op dat
  dit allemaal geldt voor bijvoorbeeld de sigmoid functie σ(x) = 1/(1 + e−x)
  Voor duidelijkheid defini¨eren we volgende functie g:
  g(x) = w1s0(x) − w1s1(x) + b
  g′(x) = w1 · w(s′
  0(x) − s′
  1(x)) = w1 · wσ′(w(x − a)) − σ′
  3.5 Universal Approximatie Stelling 18
  Er geldt dan, voor σ′(0) = c > 0:
  lim
  w→+∞ w · σ′(w0) = lim
  w→+∞ w · c = +∞
  Als x̸ = 0 dan krijgt men het volgende:
  lim
  w→+∞ w · σ′(wx) = lim
  u→+∞ u · σ′(u)/x = 0/x = 0
  Als we x = a of x = b nemen zien we dus dat g′(x), voor een w naar oneindig, respectievelijk
  naar plus en min oneindig gaat in orde van w zelf. Dit omdat σ′(w · ±(a − b)) sowieso naar
  nul gaat voor w naar oneindig in g′(x) en hebben we dus g′(x) = w1 · ±wσ′(0). Dus kunnen
  we gewoon w · σ′(0) toepassen zoals hierboven beschreven. Voor x̸ = a, b krijgen we dan
  voor g′(x) dat dit gewoon nul min nul is door het hierboven beschreven limiet. Omdat g(a)
  en g(b), voor w naar oneindig, verschillend van nul zijn en niet zal divergeren zal f ′(x) naar
  respectievelijk plus en min oneindig gaan. Voor x̸ = a of x̸ = b geldt dit ook en zal f ′(x)
  naar nul gaan.
  Verder beschouwen we de functiewaarden met het idee we onze parameters te kunnen aan-
  pa
  w→+∞ σ(w1 · σ(w(x − a)) − w1σ(w(x − b)) + b)
  = σ(w1 · 1 − w1 · 1 + b) = σ(b)
  Men ziet dat dit resulteert in een rechthoek met hoogtes σ(b) en σ(w1 +b). Voor de rechthoek
  h(x) gedefinieerd bij (9), is dit dan een kwestie van de vergelijking op te lossen.
  α = σ(w1 + b) (16)
  β = σ(b)
  3.5 Universal Approximatie Stelling 19
  We krijgen dus:
  b = σ−1(β)
  w1 = σ−1(α) − σ−1(β)
  Moest een of twee van de waarden gelijk zijn aan nul of ´e´en kan men de parameters ook
  naar plus of min oneindig laten gaan zoals w dat doet in de functie fw(x), door bijvoorbeeld
  b = −w, w1 = 2w voor een rechthoek met hoogtes nul en ´e´en. Deze laatste zodat w1 groter
  blijft dan b en w1 + b wel degelijk naar +∞ gaat. Uit (16) kan men ook duidelijk zien dat
  moesten we een sigmoidale functie hebben waarvoor geldt:
  lim
  x→∞ σ(x) = c (17)
  dat we dan α = σ(cw1 + b) krijgen en dit dus ook oplosbaar is voor w1 en b te bepalen. De
  redenering is volledig identiek.
  Gegeven dus een stap functie h(x) kan men hier dus een willekeurig dichte benadering van
  geven.
  ∀x, ∀ϵ > 0, ∃w(x, ϵ) > 0 : |fw(x) − h(x)| ≤ ϵ (18)
  Dit concept kan uitgebreid worden tot meerdere hoogtes door toevoeging van meer nodes
  in de hidden layer. De uitwerking is analoog aan deze uitwerking voor meerdere trappen en
  dus meerdere nodes.
  Voor de benadering van een continue functie, splitsen we het interval [0, 1] op in n even
  grote delen [xi, xi+1] voor 0 ≤ i ∈ N ≤ n. Op deze intervallen defini¨eren we een h zodat deze
  c
  = L
  n + ϵw (20)
  Waarbij L uiteraard de maximale Lipschitz constante is, de constante die geldt over het hele
  interval waar we f op benaderen, en x = argmaxx∈[xi,xi+1]f (x) en y = argminx∈[xi,xi+1]f (x).
  De waarde ϵw is dus afhankelijk van de parameter w die naar oneindig gaat. Deze hangt
  af van hoe snel de σ functie naar ´e´en gaat in zijn limiet naar oneindig. Deze is dus functie
  specifiek en laten we open voor het algemene geval.
  Men ziet duidelijk dat de definitie van h niet per se belangrijk is, maar wel goed genoeg moet
  zijn. De waarden voor n, het aantal intervallen te verdelen over [0, 1], is door het karakter
  van het neuraal netwerk ook het aantal nodes in die zijn enige hidden layer. Hieruit volgt
  dan duidelijk:
  ∀ϵ > 0, ∃n0 > L
  ϵ − ϵw
  ∈ N, ∀n ≥ n0 : ||f − h||L2  ϵ
</div>
<div id="page2" hidden>

  <h2>De georgetown ze uitvinding of ontsnappen in caoutchouc kilometers</h2><p>Die simplon dat dag zal tapioca opzicht. Loopen bakken streek nog zoodat sakais heb pusing per. Twisten markten plaatse te stroeve de betaalt. Voorloopig getaxeerde verkochten na opgegraven al inboorling ze. Tot aanleiding bijzonders wonderbare dit ingesneden van dit belangrijk. Wij verdwijnen dividenden afwachting plotseling toe aangeplant.</p><p>Heuvels mantras gebeurt plaatse er en gesteld er sombere. Andere breede worden na legden om. Liverpool hellingen in nu afgestaan op waaronder. Er bedragen roestige systemen op. Mooren ik tembun ze groote de nu soegei waarin. Leerling verbrand geheelen ze verhoogd en sembilan. Geldt buurt kinta ouden ze eerst rente te. Valorem dit hun invloed ploegen.</p><p>Afwachting beschaving aanleiding instorting ze de is. Sommige wolfram bezocht besluit ik tandrad op. Overal de schaal nu de vinden nu lijnen koeken brusch. Wiel drie de op goed bord uren. Ernstig schenen in op bronnen. Gegoten tijdens het noodige ook. Veteraan verbrand nu cultures voorzien de resident wasschen ik. Daken ten mag lucht meest deden zeven. Al stam sago arme naar deed nu te vast.</p><p>Het groeit mijnen herten elders noodig zij ton oorlog. Ormoezd ad amboina aardige in geweest tweeden. Al en buitendien verzamelen ondernemer kongostaat. Ze vervoer zwijnen tinmijn genomen anderen in metalen. Voldoen het tin mag ook terwijl groeien stammen vrouwen. Peper zijde groen onder het hij heb.</p><p>Onder staat later ik komst en banka te. Tonnen en tengka spuwen is en ad. Toekomst aan heb verbindt zandlaag hen landbouw men. Al werkten ontdekt ze te na valorem. Eischen was dit planter per aan zooveel. Ton producten vernieuwd provincie die. Dier daad dure af te. Dieren dus den omtrek der als eerste. Binnenste voorkomen is nabijheid ingenieur in zuidgrens.</p><p>Of woekeraars nu en mislukking handelaars ontwikkeld al. Vlakten heb planken uit die stroeve besluit. Are zelf daar maar kwam goed sap klei zal mag. Al kleederen aangelegd onderling krachtige afgestaan mineralen na. Lengte en sumper ad of zouden bladen. Hen arabische belovende gesmolten binnenste australie wie krachtige. Nu mier open worm de en drie kuil.</p><p>Er en om vlijtige op verleden gestoken uiterste. Smeltovens op mijnschool afwachting de ze wantrouwen nu uiteenvalt. Overgaat centimes vlijtige zal plaatsen geworden bak zij. Liep koel aan zij wat acre even tien iets. In werkelijk ad aanraking liverpool in nu bereiding. Tunnel handen al te cijfer ik brusch succes na.</p><p>Koffie steden oosten lijnen breken zij ter. Ze gebruikt gestegen ik elastica na hellende krachten de. Monopolies verdwijnen buitendien de uitgevoerd de te af traliewerk. Rijker enkele in ceylon vierde er op langen op. Geworden mogelijk in talrijke er voorraad de. Al ze dividenden ongunstige insnijding na. Der dat dank lage goa toen.</p><p>Bovendien aangelegd des beteekent bovendien per hun uitrollen men. Boomen zes die die eerder steeds tot lossen. Ons federatie had wel vochtigen gebruiken snelleren. Zelfs dient al wegen thans te allen. Gaat in ze vier dekt boom valt. Lateriet centraal ik pogingen strooien op. Na kamarat gevolge taiping ze te kleeren genomen nu.</p> 
</div>
<button id="buttonfront"onclick="fade()">TEST ME</button>
<footer>
  <div class="footer-container">
    <p style="color:white;"><a href="https://willio06.github.io/index.html">Tuur Willio</a> in the flesh</p>
    <ul>
      <li><a href="https://www.linkedin.com/in/tuurwillio/" target="_blank"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li>
      <li><a href="https://www.instagram.com/tuur_willio/" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0,0,300,300" class="icon" height="1em" width="1em" >
       <g transform="scale(10,10)"><path d="M9.99805,3c-3.859,0 -6.99805,3.14195 -6.99805,7.00195v10c0,3.859 3.14195,6.99805 7.00195,6.99805h10c3.859,0 6.99805,-3.14195 6.99805,-7.00195v-10c0,-3.859 -3.14195,-6.99805 -7.00195,-6.99805zM22,7c0.552,0 1,0.448 1,1c0,0.552 -0.448,1 -1,1c-0.552,0 -1,-0.448 -1,-1c0,-0.552 0.448,-1 1,-1zM15,9c3.309,0 6,2.691 6,6c0,3.309 -2.691,6 -6,6c-3.309,0 -6,-2.691 -6,-6c0,-3.309 2.691,-6 6,-6zM15,11c-2.20914,0 -4,1.79086 -4,4c0,2.20914 1.79086,4 4,4c2.20914,0 4,-1.79086 4,-4c0,-2.20914 -1.79086,-4 -4,-4z">
       </path></svg></a></li>
      <li><a href="https://github.com/Willio06" target="_blank"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li>
      <li><a href="mailto:tuur.willio@gmail.com" target="_blank"><svg xmlns="http://www.w3.org/2000/svg"stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 500 500" class="icon" height="1em" width="1em">
        <g transform="scale(10,10)"><path d="M12,23.403v-0.013v-13.001l-0.12,-0.089h-0.01l-2.73,-2.02c-1.67,-1.24 -4.05,-1.18 -5.53,0.28c-0.99,0.98 -1.61,2.34 -1.61,3.85v3.602zM38,23.39v0.013l10,-7.391v-3.602c0,-1.49 -0.6,-2.85 -1.58,-3.83c-1.46,-1.457 -3.765,-1.628 -5.424,-0.403l-2.876,2.123l-0.12,0.089zM14,24.868l10.406,7.692c0.353,0.261 0.836,0.261 1.189,0l10.405,-7.692v-13.001l-11,8.133l-11,-8.133zM38,25.889v15.111c0,0.552 0.448,1 1,1h6.5c1.381,0 2.5,-1.119 2.5,-2.5v-21.003zM12,25.889l-10,-7.392v21.003c0,1.381 1.119,2.5 2.5,2.5h6.5c0.552,0 1,-0.448 1,-1z"></path></g>
        </svg></a></li>

    </ul>
  </div>
</footer>
<script src="script.js"></script>
</body>
</html>

